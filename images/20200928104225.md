[TOC]



## 1. 指标评估

​		评估指标用于反映模型效果。在预测问题中，要评估模型的效果，就需要将模型预测结果f (X)
和真实标注Y进行比较， 评估指标定义为f(X) 和Y的函数。

​		通常，线下使用的是机器学习评估指标，线上使用的是业务指标。如果线下指标和线上指标
不同， 则可能会出现线下指标变好而线上指标变差的现象。为此，在一个新问题的开始阶段，都
会进行多轮模型迭代，来探索与线上业务指标一致的线下指标， 尽可能使线下指标的变化趋势跟
线上指标一致。没有一个跟线上一致的线下指标，意味着线下指标没有参考价值， 想、判断此次试
验是否有效，只能上线实验。而上线实验成本远高于离线实验成本，通常需要在线实验较长时间
并对效果进行可信度检验(如t-test ) 之后才能得出结论，这必然会导致模型迭代进度变慢。

​		评估指标根据任务类型分类，可分为分类指标、回归指标、聚类指标和排序指标等。

下面将介绍一些常用的评估指标以及它们适用场景。

### 1.1 混淆矩阵(confusion matrix)

混淆矩阵是一种 NxN 表格，用于总结分类模型的预测效果；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在二元问题中，N=2。

![æ··æ·ç©éµ](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/Confusion_matrix.png)

例如，下面显示了一个二元分类问题的混淆矩阵示例：

|                    | 肿瘤（预测的标签） | 非肿瘤（预测的标签） |                                                 |
| :----------------- | :----------------- | -------------------- | ----------------------------------------------- |
| 肿瘤（实际标签）   | 90                 | 10                   | 90/(90+10)=9/10                                 |
| 非肿瘤（实际标签） | 10                 | 990                  | 990/(990+10)=99/100                             |
|                    | 90/(90+10)=9/10    | 990/(990+10)=99/100  | 准确率（Accuracy）(90+990)/(90+10+10+990)=10/11 |

上面的混淆矩阵显示，

在 100个实际有肿瘤的样本中，该模型正确地将 90个归类为有肿瘤（90个正例），错误地将 10 个归类为没有肿瘤（10个假负例）。

同样，在 1000个实际没有肿瘤的样本中，模型归类正确的有 990个（452 个负例），归类错误的有 10个（10 个假正例）。





### 1.1.2 指标计算

精确率和召回率多用于二分类问题，可结合棍淆矩阵介绍：

![image-20200916194059093](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/image-20200916194059093.png)

TP (真正， True Positive )表示真实结果为正例，预测结果也是正例; 

FP (假正， FalsePositive) 表示真实结果为负例，预测结果却是正例; 

TN (真负， True Negative) 表示真实结果为正例，预测结果却是负例; 

FN (假负， False Negative ) 表示真实结果为负例，预测结果也是负例。

显然， TP + FP + FN + TN = 样本总数。

#### 准确率（Accuracy）

**(TP+TN) / (TP+ FP+FN+TN)**

准确率即预测正确的比例;

有时候，模型准确性并不能评价一个算法的好坏。

一般情况下，评估一个模型的好坏的指标是：查准率（Precision）和召回率（Recall）。

#### 精确率( Precision) (也叫查准率)

**TP / TP+ FP**

精准率反应的是预测准确的数量，即预测为正的样本中有多少是真正的正样本，

预测准确的次数越多，这个数值就越大。精准率是针对我们预测结果而言的，

它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，

一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，

#### 召回率( Recall)

**TP / TP+ FN** 

召回率反应的是预测准确的概率，它表示的是样本中的正例有多少被预测正确了，

如果预测百分之百地准确，那这个数值就会很高。

召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。

也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。

#### F1 Score

**2 /( (1/Precision)+(1/Recall) )**

计算公式是：精确率与召回率的平均调和；

F1分数是用于整体评价的指标。

精确率和召回率是相互影响的，精确率升高则召回率下降，召回率升高则精确率下降，如果需要兼顾二者，就需要精确率、召回率的结合F1 Score。

典型地，如果查准率或召回率有一个为0，那么F1分数将会为0。

而理想的情况下，查准率和召回率都为1，则算出来的F1分数为1。

因此，F1分数越高，则说明一个算法的表现越好。

#### P-R曲线（Precision-Recall Curve）

精准率-召回率曲线也叫PR曲线，P-R曲线是描述精确率和召回率变化的曲线；

P-R曲线以Recall为X轴坐标，以Precision为Y轴坐标，通过对模型算法设定不同的阈值会得到不同的precision和recall值，将这些序列绘制到直角坐标系上就得到了PR曲线，PR曲线下的面积为1时则说明模型算法性能最为理想。

<img src="https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200917141856.png" alt="p-r" style="zoom:50%;" />

#### ROC（Receiver Operating Characteristic）

ROC曲线以 FPR 为横轴，TPR 为纵轴，绘制出来的曲线就是ROC曲线。

TPR：在所有实际为正例的样本中，被正确地判断为正例之比率。
TPR =TP / (TP + FN)
FPR：在所有实际为负例的样本中，被错误地判断为正例之比率。
FPR = FP / (FP + TN)

#### AUC(Area Under Curve)

AUC 即ROC曲线下的面积，ROC曲线下与坐标轴围成的面积，显然这个面积的数值不会大于1。

又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。

AUC越接近1.0，检测方法真实性越高；

等于0.5时，则真实性最低，无应用价值。

![roc.png](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200917143255.png)



#### KS（Kolmogorov-Smirnov）

KS曲线是把TPR和FPR都作为纵坐标，而样本数作为横坐标。

通过衡量好坏样本累计分布之间的差值，来评估模型的风险区分能力。

KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计分部之间的差值。 

好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。

KS不同代表的不同情况，一般情况KS值越大，模型的区分能力越强，但是也不是越大模型效果就越好，如果 KS过大，模型可能存在异常，所以当KS值过高可能需要检查模型是否过拟合。

以下为KS值对应的模型情况， 但此对应不是唯一的，只代表大致趋势。

| ks值     | 含义                                   |
| -------- | -------------------------------------- |
| <  0     | 模型错误                               |
| 0~0.2    | 模型没有区分能力，模型预测能力较差     |
| 0,2~0.3  | 模型具有一定区分能力，勉强可以接受     |
| 0.3~0.75 | 模型具有较强的区分能力，模型预测性较好 |
| >0.75    | 一般表示模型有异常，但不绝对           |



**KS的计算步骤如下：** 
1） 计算每个评分区间的好坏账户数。 
2） 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 
3）计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。

![image-20200921115755463](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200921115942.png)

​								[【模型 区分度】神秘的KS值和GINI系数](]http://www.jeepxie.net/article/130942.html)

作图步骤：

1） 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）-----这就是截断点依次选取的顺序

2）按顺序选取截断点，并计算TPR和FPR ---也可以只选取n个截断点，分别在1/n，2/n，3/n等位置

3）横轴为样本的占比百分比（最大100%），纵轴分别为TPR和FPR，可以得到KS曲线

4）TPR和FPR曲线分隔最开的位置就是最好的”截断点“，最大间隔距离就是KS值，通常>0.2即可认为模型有比较好偶的预测准确性



### 1.1.3 代码实现评估指标

#### accuracy_score、precision_score、recall_score、f1_score

```python
from sklearn import metrics 
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.metrics import accuracy_score
#分类结果
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 1, 1]
print("accuracy_score:", accuracy_score(y_true, y_pred))
print("precision_score:", metrics.precision_score(y_true, y_pred))
print("recall_score:", metrics.recall_score(y_true, y_pred))
print("f1_score:", metrics.f1_score(y_true, y_pred))
print("f0.5_score:", metrics.fbeta_score(y_true, y_pred, beta=0.5))
print("f2_score:", metrics.fbeta_score(y_true, y_pred, beta=2.0))
```

accuracy_score: 0.5
precision_score: 1.0
recall_score: 0.3333333333333333
f1_score: 0.5
f0.5_score: 0.7142857142857143
f2_score: 0.3846153846153846

#### auc

```python
# auc
import numpy as np
from sklearn import metrics
y = np.array([1, 1, 2, 2])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
metrics.auc(fpr, tpr)

# 当类别满足{0, 1}时，可直接使用metrics.roc_auc_score(y,pred)
metrics.roc_auc_score(y,pred)
```

0.75

#### P-R曲线

```python
## P-R曲线
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
y_pred = [0, 1, 1, 0, 1, 1, 0, 1, 1, 1]
y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]
precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
plt.plot(precision, recall)
```

![image-20200917145551998](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200917145552.png)

#### ROC曲线

```python
## ROC曲线
from sklearn.metrics import roc_curve
y_pred = [0, 1, 1, 0, 1, 1, 0, 1, 1, 1]
y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]
FPR,TPR,thresholds=roc_curve(y_true, y_pred)
plt.title('ROC')
plt.plot(FPR, TPR,'b')
plt.plot([0,1],[0,1],'r--')
plt.ylabel('TPR')
plt.xlabel('FPR')
```

![image-20200917145632024](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200917145632.png)

#### KS曲线

```python
## KS值 在实际操作时往往使用ROC曲线配合求出KS值
sns.set(font='SimHei')                        #解决Seaborn中文显示的问题
matplotlib.rcParams['font.family']='SimHei'
plt.rcParams['font.sans-serif'] = ['SimHei']  #中文字体设置-黑体
plt.rcParams['axes.unicode_minus'] = False   #解决保存图像是负号'-'显示为方块的问题

from sklearn.metrics import roc_curve


data = {'y_label':[1,1,1,1,1,1,0,0,0,0,0,0],
    'y_pred':[0.5,0.6,0.7,0.6,0.6,0.8,0.4,0.2,0.1,0.4,0.3,0.9]}
df = pd.DataFrame(data)
fpr, tpr, thresholds= roc_curve(df.y_label, df.y_pred)
ks_value = max(abs(fpr-tpr))
print('KS值：', ks_value)

# 画图，画出曲线
plt.plot(fpr, label='bad')
plt.plot(tpr, label='good')
plt.plot(abs(fpr-tpr), label='diff')
# 标记ks
x = np.argwhere(abs(fpr-tpr) == ks_value)[0, 0]
plt.plot((x, x), (0, ks_value), label='ks - {:.2f}'.format(ks_value), color='r', marker='o', markerfacecolor='r', markersize=5)
plt.scatter((x, x), (0, ks_value), color='r')
plt.legend()
plt.show()
```

KS值：0.8333333333333334

![image-20200922181021060](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200922181021.png)

计算详解可参考：[利用Python计算KS的实例详解](https://www.zhangshengrong.com/p/AvN6YEZkam/)



## 2. sklearn.metrics函数

### 2.1 sklearn.metrics.roc_curve

[官方文档](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc_curve)



```python
sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)
```

参数说明：

```python
1）y_true
y_true（array, shape = [n_samples])
# 真正的二进制标签。如果标签既不是{-1，1}也不是{0，1}，则应该明确给出pos_label。
# 数组，存储数据的标签，维度就是样本数，形如[0,1,1,0,1...]这样的，也可以是-1和1，只要有两个值
2）y_score
y_score（array, shape = [n_samples]）
# 目标分数可以是肯定类别的概率估计值，置信度值或决策的非阈值度量（如某些分类器上的“ decision_function”所返回）。
# 数组，存储数据的预测概率值，维度也是样本数，形如[0.38,0.5,0.8]这样的
3）pos_label
pos_label（int or str, default=None）
# 正类的标签，当pos_label = None时，如果y_true在{-1，1}或{0，1}中，则pos_label设置为1，否则将引发错误。
# 整型或字符串，当y_true中只有一个值时，比如都是1或者都是0，无法判断哪个是正样本，需要用一个数字或字符串指出
4）sample_weight
sample_weight（array-like of shape (n_samples,), default=None）
采样权重
5）drop_intermediate
drop_intermediate（boolean, optional (default=True)）
# 是否降低一些不会出现在绘制的ROC曲线上的次优阈值。即丢掉一些阈值，这对于创建更浅的ROC曲线很有用。
```

返回值：

一共三个，分别是fpr,tpr,thresholds

```python
fpr（false positive rates）
fpr（array, shape = [>2]）
# Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].
# 数组，随阈值上涨的假阳性率
tpr（true positive rates）
tpr（array, shape = [>2]）
# Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].
# 数组，随阈值上涨的真正例率
thresholds
thresholds（array, shape = [n_thresholds]）
# Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1.
# 数组，对预测值排序后的score列表，作为阈值，排序从大到小
```

举例：

```python
import numpy as np
from sklearn import metrics
y = np.array([1, 1, 2, 2])
scores = np.array([0.1, 0.4, 0.35, 0.8])
# pos_label = 2 即表示标签为2的是正样本，其余的都是负样本，因为这个只能做二分类；
fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
fpr
tpr
thresholds
```

![image-20200917151118959](https://cdn.jsdelivr.net/gh/jixiang-wang/picture/images/20200917151119.png)

详细计算过程：

```
fpr（假阳性率）= 负例被误判为正样本的数量 / 负样本数
fpr（真正例率）= 正例被判为正样本的数量 / 正样本数
thresholds = 对预测值降序后的score列表

阈值选择从大到小: 

1）取0.8为1，其余为0 预测值为：[0,0,0,1] 
fpr=反例中预测为正例的个数/实际反例个数=0/2=0 
tpr=正例中预测为正例的个数/实际正例个数=1/2=0.5 

2）取0.4为1，预测为：[0,1,0,1] 
fpr=1/2=0.5 
tpr=1/2=0.5 

3）取0.35为1，预测为:[0,1,1,1] 
fpr=1/2=0.5 
tpr=2/2=1

4）取0.1为1，其余为0，预测值为：[1, 1, 1, 1]
fpr=2/2=1
tpr=2/2=1

fpr = [0, 0.5, 0.5, 1]

tpr = [0.5, 0.5, 1, 1]

thresholds: [ 0.8 ,  0.4 ,  0.35,  0.1 ]   即对预测值降序排序后的score列表
```



### 2.1 metrics.auc

```python
sklearn.metrics.auc(x, y)
```

举例：

```python
import numpy as np
from sklearn import metrics
y = np.array([1, 1, 2, 2])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
metrics.auc(fpr, tpr)
```

0.75



注意：

当类别满足{0, 1}时，可直接使用metrics.roc_auc_score(y,pred)

```python
import numpy as np
from sklearn import metrics
# y = np.array([1, 1, 2, 2])
y=np.array([0,0,1,1])  
pred = np.array([0.1, 0.4, 0.35, 0.8])
# fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
metrics.roc_auc_score(y,pred)
```

0.75



其他关于metrics.auc的使用案例，可参考;

1、Python metrics.auc方法代碼示例

https://vimsky.com/zh-tw/examples/detail/python-method-sklearn.metrics.auc.html



### 2.3 metrics.roc_auc_score

sklearn.metrics.roc_auc_score

```python
sklearn.metrics.roc_auc_score（y_true，y_score，*，average ='macro'，sample_weight = None，max_fpr = None，multi_class ='raise'，labels = None ）
```

参数：

```
y_truearray-like of shape (n_samples,) or (n_samples, n_classes)
True labels or binary label indicators. The binary and multiclass cases expect labels with shape (n_samples,) while the multilabel case expects binary label indicators with shape (n_samples, n_classes).

y_scorearray-like of shape (n_samples,) or (n_samples, n_classes)
Target scores. In the binary and multilabel cases, these can be either probability estimates or non-thresholded decision values (as returned by decision_function on some classifiers). In the multiclass case, these must be probability estimates which sum to 1. The binary case expects a shape (n_samples,), and the scores must be the scores of the class with the greater label. The multiclass and multilabel cases expect a shape (n_samples, n_classes). In the multiclass case, the order of the class scores must correspond to the order of labels, if provided, or else to the numerical or lexicographical order of the labels in y_true.

average{‘micro’, ‘macro’, ‘samples’, ‘weighted’} or None, default=’macro’
If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: Note: multiclass ROC AUC currently only handles the ‘macro’ and ‘weighted’ averages.

'micro':
Calculate metrics globally by considering each element of the label indicator matrix as a label.

'macro':
Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.

'weighted':
Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label).

'samples':
Calculate metrics for each instance, and find their average.

Will be ignored when y_true is binary.

sample_weightarray-like of shape (n_samples,), default=None
Sample weights.

max_fprfloat > 0 and <= 1, default=None
If not None, the standardized partial AUC [2] over the range [0, max_fpr] is returned. For the multiclass case, max_fpr, should be either equal to None or 1.0 as AUC ROC partial computation currently is not supported for multiclass.

multi_class{‘raise’, ‘ovr’, ‘ovo’}, default=’raise’
Multiclass only. Determines the type of configuration to use. The default value raises an error, so either 'ovr' or 'ovo' must be passed explicitly.

'ovr':
Computes the AUC of each class against the rest [3] [4]. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when average == 'macro', because class imbalance affects the composition of each of the ‘rest’ groupings.

'ovo':
Computes the average AUC of all possible pairwise combinations of classes [5]. Insensitive to class imbalance when average == 'macro'.

labelsarray-like of shape (n_classes,), default=None
Multiclass only. List of labels that index the classes in y_score. If None, the numerical or lexicographical order of the labels in y_true is used.
```

返回：

float数值





参考链接：

[【模型 区分度】神秘的KS值和GINI系数](]http://www.jeepxie.net/article/130942.html)



